{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ghactivity'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.setdefault('AWS_PROFILE', 'ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-08 21:24:04   33989636 2022-06-05-0.json.gz\n",
      "2022-06-08 21:44:13   39078924 2022-06-05-1.json.gz\n",
      "2022-06-14 19:10:52   45518853 2022-06-05-10.json.gz\n",
      "2022-06-14 19:16:54   40533549 2022-06-05-11.json.gz\n",
      "2022-06-13 10:21:47   34733178 2022-06-05-2.json.gz\n",
      "2022-06-13 17:09:33   36298891 2022-06-05-3.json.gz\n",
      "2022-06-13 18:00:14   28220189 2022-06-05-4.json.gz\n",
      "2022-06-13 18:01:13   30436488 2022-06-05-5.json.gz\n",
      "2022-06-13 18:10:06   31640337 2022-06-05-6.json.gz\n",
      "2022-06-13 18:33:41   33084474 2022-06-05-7.json.gz\n",
      "2022-06-14 09:49:19   41575819 2022-06-05-8.json.gz\n",
      "2022-06-14 17:47:37   38022512 2022-06-05-9.json.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://aigithub/landing/ghactivity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-bed449c4-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-66907c10-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-a5549936-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-8acac0f4-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-b244c292-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-7b27f68a-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-91a055f6-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-b806405c-edf5-11ec-b327-e69f877c2171.snappy.parquet\n",
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-55c28f9a-edf5-11ec-b327-e69f877c2171.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://aigithub/raw/ghactivity/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'aigithub'\n",
    "file_name = '2022-06-05-0.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\n",
    "    f's3://{bucket_name}/landing/ghactivity/{file_name}',\n",
    "    lines=True,\n",
    "    orient='records'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'type', 'actor', 'repo', 'payload', 'public', 'created_at',\n",
       "       'org'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = file_name.split('-')[0]\n",
    "month = file_name.split('-')[1]\n",
    "dayofmonth = file_name.split('-')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "target_file_name = f'{uuid.uuid1()}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d55db3a6-ee1d-11ec-b1c2-e69f877c2170.snappy.parquet'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Axis'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Level | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Drop specified labels from rows or columns.\n",
      "\n",
      "Remove rows or columns by specifying label names and corresponding\n",
      "axis, or by specifying directly index or column names. When using a\n",
      "multi-index, labels on different levels can be removed by specifying\n",
      "the level. See the `user guide <advanced.shown_levels>`\n",
      "for more information about the now unused levels.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "labels : single label or list-like\n",
      "    Index or column labels to drop. A tuple will be used as a single\n",
      "    label and not treated as a list-like.\n",
      "axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "    Whether to drop labels from the index (0 or 'index') or\n",
      "    columns (1 or 'columns').\n",
      "index : single label or list-like\n",
      "    Alternative to specifying axis (``labels, axis=0``\n",
      "    is equivalent to ``index=labels``).\n",
      "columns : single label or list-like\n",
      "    Alternative to specifying axis (``labels, axis=1``\n",
      "    is equivalent to ``columns=labels``).\n",
      "level : int or level name, optional\n",
      "    For MultiIndex, level from which the labels will be removed.\n",
      "inplace : bool, default False\n",
      "    If False, return a copy. Otherwise, do operation\n",
      "    inplace and return None.\n",
      "errors : {'ignore', 'raise'}, default 'raise'\n",
      "    If 'ignore', suppress error and only existing labels are\n",
      "    dropped.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or None\n",
      "    DataFrame without the removed index or column labels or\n",
      "    None if ``inplace=True``.\n",
      "\n",
      "Raises\n",
      "------\n",
      "KeyError\n",
      "    If any of the labels is not found in the selected axis.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.loc : Label-location based indexer for selection by label.\n",
      "DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      "    where (all or any) data are missing.\n",
      "DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
      "    removed, optionally only considering certain columns.\n",
      "Series.drop : Return Series with specified index labels removed.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
      "...                   columns=['A', 'B', 'C', 'D'])\n",
      ">>> df\n",
      "   A  B   C   D\n",
      "0  0  1   2   3\n",
      "1  4  5   6   7\n",
      "2  8  9  10  11\n",
      "\n",
      "Drop columns\n",
      "\n",
      ">>> df.drop(['B', 'C'], axis=1)\n",
      "   A   D\n",
      "0  0   3\n",
      "1  4   7\n",
      "2  8  11\n",
      "\n",
      ">>> df.drop(columns=['B', 'C'])\n",
      "   A   D\n",
      "0  0   3\n",
      "1  4   7\n",
      "2  8  11\n",
      "\n",
      "Drop a row by index\n",
      "\n",
      ">>> df.drop([0, 1])\n",
      "   A  B   C   D\n",
      "2  8  9  10  11\n",
      "\n",
      "Drop columns and/or rows of MultiIndex DataFrame\n",
      "\n",
      ">>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n",
      "...                              ['speed', 'weight', 'length']],\n",
      "...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      "...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      ">>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
      "...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
      "...                         [250, 150], [1.5, 0.8], [320, 250],\n",
      "...                         [1, 0.8], [0.3, 0.2]])\n",
      ">>> df\n",
      "                big     small\n",
      "lama    speed   45.0    30.0\n",
      "        weight  200.0   100.0\n",
      "        length  1.5     1.0\n",
      "cow     speed   30.0    20.0\n",
      "        weight  250.0   150.0\n",
      "        length  1.5     0.8\n",
      "falcon  speed   320.0   250.0\n",
      "        weight  1.0     0.8\n",
      "        length  0.3     0.2\n",
      "\n",
      "Drop a specific index combination from the MultiIndex\n",
      "DataFrame, i.e., drop the combination ``'falcon'`` and\n",
      "``'weight'``, which deletes only the corresponding row\n",
      "\n",
      ">>> df.drop(index=('falcon', 'weight'))\n",
      "                big     small\n",
      "lama    speed   45.0    30.0\n",
      "        weight  200.0   100.0\n",
      "        length  1.5     1.0\n",
      "cow     speed   30.0    20.0\n",
      "        weight  250.0   150.0\n",
      "        length  1.5     0.8\n",
      "falcon  speed   320.0   250.0\n",
      "        length  0.3     0.2\n",
      "\n",
      ">>> df.drop(index='cow', columns='small')\n",
      "                big\n",
      "lama    speed   45.0\n",
      "        weight  200.0\n",
      "        length  1.5\n",
      "falcon  speed   320.0\n",
      "        weight  1.0\n",
      "        length  0.3\n",
      "\n",
      ">>> df.drop(index='length', level=1)\n",
      "                big     small\n",
      "lama    speed   45.0    30.0\n",
      "        weight  200.0   100.0\n",
      "cow     speed   30.0    20.0\n",
      "        weight  250.0   150.0\n",
      "falcon  speed   320.0   250.0\n",
      "        weight  1.0     0.8\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Projects/Internal/bootcamp/itversity-material/ghactivity-aws/ga-venv/lib/python3.9/site-packages/pandas/core/frame.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "df.drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | WriteBuffer[bytes] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'snappy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpartition_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'bytes | None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Write a DataFrame to the binary parquet format.\n",
      "\n",
      "This function writes the dataframe as a `parquet file\n",
      "<https://parquet.apache.org/>`_. You can choose different parquet\n",
      "backends, and have the option of compression. See\n",
      ":ref:`the user guide <io.parquet>` for more details.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      "    object implementing a binary ``write()`` function. If None, the result is\n",
      "    returned as bytes. If a string or path, it will be used as Root Directory\n",
      "    path when writing a partitioned dataset.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "    Previously this was \"fname\"\n",
      "\n",
      "engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
      "    Parquet library to use. If 'auto', then the option\n",
      "    ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n",
      "    behavior is to try 'pyarrow', falling back to 'fastparquet' if\n",
      "    'pyarrow' is unavailable.\n",
      "compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n",
      "    Name of the compression to use. Use ``None`` for no compression.\n",
      "index : bool, default None\n",
      "    If ``True``, include the dataframe's index(es) in the file output.\n",
      "    If ``False``, they will not be written to the file.\n",
      "    If ``None``, similar to ``True`` the dataframe's index(es)\n",
      "    will be saved. However, instead of being saved as values,\n",
      "    the RangeIndex will be stored as a range in the metadata so it\n",
      "    doesn't require much space and is faster. Other indexes will\n",
      "    be included as columns in the file output.\n",
      "partition_cols : list, optional, default None\n",
      "    Column names by which to partition the dataset.\n",
      "    Columns are partitioned in the order they are given.\n",
      "    Must be None if path is not a string.\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "**kwargs\n",
      "    Additional arguments passed to the parquet library. See\n",
      "    :ref:`pandas io <io.parquet>` for more details.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "bytes if no path argument is provided else None\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_parquet : Read a parquet file.\n",
      "DataFrame.to_csv : Write a csv file.\n",
      "DataFrame.to_sql : Write to a sql table.\n",
      "DataFrame.to_hdf : Write to hdf.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This function requires either the `fastparquet\n",
      "<https://pypi.org/project/fastparquet>`_ or `pyarrow\n",
      "<https://arrow.apache.org/docs/python/>`_ library.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n",
      ">>> df.to_parquet('df.parquet.gzip',\n",
      "...               compression='gzip')  # doctest: +SKIP\n",
      ">>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n",
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n",
      "\n",
      "If you want to get a buffer to the parquet content you can use a io.BytesIO\n",
      "object, as long as you don't use partition_cols, which creates multiple files.\n",
      "\n",
      ">>> import io\n",
      ">>> f = io.BytesIO()\n",
      ">>> df.to_parquet(f)\n",
      ">>> f.seek(0)\n",
      "0\n",
      ">>> content = f.read()\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Projects/Internal/bootcamp/itversity-material/ghactivity-aws/ga-venv/lib/python3.9/site-packages/pandas/core/frame.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "df.to_parquet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lift and shift model might run into out of memory issues\n",
    "# as the operation is both memory and CPU Intensive\n",
    "# I have tested this as lambda and it is failing even with 10 GB memory (which is max)\n",
    "df.drop(columns=['payload']). \\\n",
    "    to_parquet(\n",
    "        f's3://{bucket_name}/raw/ghactivity/year={year}/month={month}/dayofmonth={dayofmonth}/{target_file_name}',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-17 14:48:10    5655036 raw/ghactivity/year=2022/month=06/dayofmonth=05/d55db3a6-ee1d-11ec-b1c2-e69f877c2170.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://aigithub/raw/ghactivity --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of lift and shift, let us take care of uploading the data in chunks\n",
    "# Every 10K records will be converted and uploaded using parquet file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aigithub/raw/ghactivity/year=2022/month=06/dayofmonth=05/d55db3a6-ee1d-11ec-b1c2-e69f877c2170.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Deleting all files which is already uploaded\n",
    "!aws s3 rm s3://aigithub/raw/ghactivity/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'aigithub'\n",
    "file_name = '2022-06-05-0.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from json file in chunks\n",
    "# Reader type object will be created rather than Dataframe\n",
    "\n",
    "df_reader = pd.read_json(\n",
    "    f's3://{bucket_name}/landing/ghactivity/{file_name}',\n",
    "    lines=True,\n",
    "    orient='records',\n",
    "    chunksize=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.json._json.JsonReader"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in chunk 0: 10000\n",
      "The number of records in chunk 1: 10000\n",
      "The number of records in chunk 2: 10000\n",
      "The number of records in chunk 3: 10000\n",
      "The number of records in chunk 4: 10000\n",
      "The number of records in chunk 5: 10000\n",
      "The number of records in chunk 6: 10000\n",
      "The number of records in chunk 7: 10000\n",
      "The number of records in chunk 8: 9443\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through reader in streaming fashion using for loop\n",
    "# When you validate from your local machine, this will take time based on the internet speed\n",
    "for idx, df in enumerate(df_reader):\n",
    "    print(f'The number of records in chunk {idx}: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = file_name.split('-')[0]\n",
    "month = file_name.split('-')[1]\n",
    "dayofmonth = file_name.split('-')[2]\n",
    "hour = file_name.split('-')[3].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part-2022-06-05-0-b3020a16-ee20-11ec-b1c2-e69f877c2170.snappy.parquet'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file_name = f'part-{year}-{month}-{dayofmonth}-{hour}-{uuid.uuid1()}.snappy.parquet'\n",
    "target_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | WriteBuffer[bytes] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'snappy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpartition_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'bytes | None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Write a DataFrame to the binary parquet format.\n",
      "\n",
      "This function writes the dataframe as a `parquet file\n",
      "<https://parquet.apache.org/>`_. You can choose different parquet\n",
      "backends, and have the option of compression. See\n",
      ":ref:`the user guide <io.parquet>` for more details.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      "    object implementing a binary ``write()`` function. If None, the result is\n",
      "    returned as bytes. If a string or path, it will be used as Root Directory\n",
      "    path when writing a partitioned dataset.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "    Previously this was \"fname\"\n",
      "\n",
      "engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
      "    Parquet library to use. If 'auto', then the option\n",
      "    ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n",
      "    behavior is to try 'pyarrow', falling back to 'fastparquet' if\n",
      "    'pyarrow' is unavailable.\n",
      "compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n",
      "    Name of the compression to use. Use ``None`` for no compression.\n",
      "index : bool, default None\n",
      "    If ``True``, include the dataframe's index(es) in the file output.\n",
      "    If ``False``, they will not be written to the file.\n",
      "    If ``None``, similar to ``True`` the dataframe's index(es)\n",
      "    will be saved. However, instead of being saved as values,\n",
      "    the RangeIndex will be stored as a range in the metadata so it\n",
      "    doesn't require much space and is faster. Other indexes will\n",
      "    be included as columns in the file output.\n",
      "partition_cols : list, optional, default None\n",
      "    Column names by which to partition the dataset.\n",
      "    Columns are partitioned in the order they are given.\n",
      "    Must be None if path is not a string.\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "**kwargs\n",
      "    Additional arguments passed to the parquet library. See\n",
      "    :ref:`pandas io <io.parquet>` for more details.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "bytes if no path argument is provided else None\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_parquet : Read a parquet file.\n",
      "DataFrame.to_csv : Write a csv file.\n",
      "DataFrame.to_sql : Write to a sql table.\n",
      "DataFrame.to_hdf : Write to hdf.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This function requires either the `fastparquet\n",
      "<https://pypi.org/project/fastparquet>`_ or `pyarrow\n",
      "<https://arrow.apache.org/docs/python/>`_ library.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n",
      ">>> df.to_parquet('df.parquet.gzip',\n",
      "...               compression='gzip')  # doctest: +SKIP\n",
      ">>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n",
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n",
      "\n",
      "If you want to get a buffer to the parquet content you can use a io.BytesIO\n",
      "object, as long as you don't use partition_cols, which creates multiple files.\n",
      "\n",
      ">>> import io\n",
      ">>> f = io.BytesIO()\n",
      ">>> df.to_parquet(f)\n",
      ">>> f.seek(0)\n",
      "0\n",
      ">>> content = f.read()\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Projects/Internal/bootcamp/itversity-material/ghactivity-aws/ga-venv/lib/python3.9/site-packages/pandas/core/frame.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "df.to_parquet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As df_reader is already processed, if we try to process again it will fail\n",
    "# Make sure to create reader object again to process while writing to s3\n",
    "df_reader = pd.read_json(\n",
    "    f's3://{bucket_name}/landing/ghactivity/{file_name}',\n",
    "    lines=True,\n",
    "    orient='records',\n",
    "    chunksize=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 1 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 2 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 3 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 4 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 5 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 6 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 7 of size 10000 from 2022-06-05-0.json.gz\n",
      "Processing chunk 8 of size 9443 from 2022-06-05-0.json.gz\n"
     ]
    }
   ],
   "source": [
    "for idx, df in enumerate(df_reader):\n",
    "    target_file_name = f'part-{year}-{month}-{dayofmonth}-{hour}-{uuid.uuid1()}.snappy.parquet'\n",
    "    print(f'Processing chunk {idx} of size {df.shape[0]} from {file_name}')\n",
    "    df.drop(columns=['payload']). \\\n",
    "    to_parquet(\n",
    "        f's3://{bucket_name}/raw/ghactivity/year={year}/month={month}/dayofmonth={dayofmonth}/{target_file_name}',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-17 15:08:37     735024 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-425d2830-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:09:57     717761 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-726e6c50-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:10:28     690393 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-84bccb86-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:11:24     718011 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-a64fea9e-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:11:47     728950 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-b3f9908c-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:12:22     753367 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-c8d1227c-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:13:11     753795 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-e5f4e9c4-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:13:22     736682 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-eca7c21e-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n",
      "2022-06-17 15:13:37     705238 raw/ghactivity/year=2022/month=06/dayofmonth=05/part-2022-06-05-0-f5980622-ee21-11ec-b1c2-e69f877c2170.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://aigithub/raw/ghactivity/ --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ga-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bcd215c4a14989c946b0b0624b2d9cd8bc3b4ff7a1c5d476ef679e9df9c7085"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
